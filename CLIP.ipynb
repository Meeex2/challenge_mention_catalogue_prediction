{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, image_dir, processor, max_length=20):\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.images = os.listdir(image_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.processor.feature_extractor(image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        \n",
    "        # Generate text (class name from image file name)\n",
    "        label = os.path.splitext(img_name)[0]  # Remove extension\n",
    "        text = self.processor.tokenizer(\n",
    "            label,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        return image, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_dir = \"./data/DAM\"\n",
    "dataset = ImageTextDataset(\n",
    "    image_dir=image_dir,\n",
    "    processor=processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack images\n",
    "    texts = {key: torch.cat([item[1][key] for item in batch], dim=0) for key in batch[0][1]}  # Combine text components\n",
    "    return images, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vikos\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:149: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vikos\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.652546853512183\n",
      "Epoch 2, Loss: 0.6499134613864723\n",
      "Epoch 3, Loss: 0.36779743407307\n",
      "Epoch 4, Loss: 0.26217492669820786\n",
      "Epoch 5, Loss: 0.20387815919586982\n",
      "Epoch 6, Loss: 0.164385296832556\n",
      "Epoch 7, Loss: 0.142594280641997\n",
      "Epoch 8, Loss: 0.10863340174078127\n",
      "Epoch 9, Loss: 0.08928976021171815\n",
      "Epoch 10, Loss: 0.09676356619256067\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        images, texts = batch\n",
    "        \n",
    "        images = images.to(device)\n",
    "        texts = {key: value.squeeze(1).to(device) for key, value in texts.items()}\n",
    "        \n",
    "        outputs = model(**texts, pixel_values=images)\n",
    "        logits_per_image = outputs.logits_per_image  # Image-text similarity\n",
    "        logits_per_text = outputs.logits_per_text  # Text-image similarity\n",
    "        \n",
    "        labels = torch.arange(len(images)).to(device)\n",
    "        \n",
    "        loss = (criterion(logits_per_image, labels) + criterion(logits_per_text, labels)) / 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Labels: ['m 5 0 0 spaawxm 1 6 y', '\"\"\"\"\"\"\"\"\"\"\"\"!!!!!!!!']\n",
      "Image 1:\n",
      "  Actual: m 5 0 0 spaawxm 1 6 y\n",
      "  Predicted: m 5 0 0 spaawxm 1 6 y\n",
      "Image 2:\n",
      "  Actual: \"\"\"\"\"\"\"\"\"\"\"\"!!!!!!!!\n",
      "  Predicted: \"\"\"\"\"\"\"\"\"\"\"\"!!!!!!!!\n",
      "Image 3:\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, pred_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(preds):\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Actual: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdecoded_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Predicted: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdecoded_labels[pred_idx]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Accuracy calculation\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        images, texts = batch\n",
    "        \n",
    "        # Move tensors to the correct device\n",
    "        images = images.to(device)\n",
    "        texts = {key: value.squeeze(1).to(device) for key, value in texts.items()}\n",
    "        \n",
    "        # Decode the labels\n",
    "        decoded_labels = [processor.tokenizer.decode(value[0], skip_special_tokens=True) for value in texts.values()]\n",
    "        print(f\"Decoded Labels: {decoded_labels}\")\n",
    "        \n",
    "        # Model inference\n",
    "        outputs = model(**texts, pixel_values=images)\n",
    "        logits_per_image = outputs.logits_per_image  # Image-text similarity\n",
    "        \n",
    "        # Predictions\n",
    "        preds = logits_per_image.argmax(dim=-1)\n",
    "        labels = torch.arange(len(images)).to(device)\n",
    "        \n",
    "        # Print predicted and actual labels\n",
    "        for idx, pred_idx in enumerate(preds):\n",
    "            print(f\"Image {idx + 1}:\")\n",
    "            print(f\"  Actual: {decoded_labels[idx]}\")\n",
    "            print(f\"  Predicted: {decoded_labels[pred_idx]}\")\n",
    "        \n",
    "        # Accuracy calculation\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(images)\n",
    "\n",
    "print(f\"Accuracy: {correct / total:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
