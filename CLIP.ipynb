{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTextDataset(Dataset):\n",
    "    def __init__(self, image_dir, processor, max_length=20):\n",
    "        self.image_dir = image_dir\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.images = os.listdir(image_dir)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.processor.feature_extractor(image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        \n",
    "        # Generate text (class name from image file name)\n",
    "        label = os.path.splitext(img_name)[0]  # Remove extension\n",
    "        text = self.processor.tokenizer(\n",
    "            label,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        return image, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_dir = \"./data/DAM\"\n",
    "dataset = ImageTextDataset(\n",
    "    image_dir=image_dir,\n",
    "    processor=processor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack images\n",
    "    texts = {key: torch.cat([item[1][key] for item in batch], dim=0) for key in batch[0][1]}  # Combine text components\n",
    "    return images, texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Vikos\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\clip\\processing_clip.py:149: FutureWarning: `feature_extractor` is deprecated and will be removed in v5. Use `image_processor` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Vikos\\miniconda3\\envs\\pytorch\\lib\\site-packages\\transformers\\models\\clip\\modeling_clip.py:540: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.5724437483425797\n",
      "Epoch 2, Loss: 0.6113876354420322\n",
      "Epoch 3, Loss: 0.30487859942789736\n",
      "Epoch 4, Loss: 0.19437606679810876\n",
      "Epoch 5, Loss: 0.16725467194685306\n",
      "Epoch 6, Loss: 0.13028303123796467\n",
      "Epoch 7, Loss: 0.10534767182704446\n",
      "Epoch 8, Loss: 0.11539564929345901\n",
      "Epoch 9, Loss: 0.09400252566469469\n",
      "Epoch 10, Loss: 0.0796978617429562\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):  \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        images, texts = batch\n",
    "        \n",
    "        images = images.to(device)\n",
    "        texts = {key: value.squeeze(1).to(device) for key, value in texts.items()}\n",
    "        \n",
    "        outputs = model(**texts, pixel_values=images)\n",
    "        logits_per_image = outputs.logits_per_image  # Image-text similarity\n",
    "        logits_per_text = outputs.logits_per_text  # Text-image similarity\n",
    "        \n",
    "        labels = torch.arange(len(images)).to(device)\n",
    "        \n",
    "        loss = (criterion(logits_per_image, labels) + criterion(logits_per_text, labels)) / 2\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Labels: 9 4 3 t 0 5 cp 4 3 0 x 0 8 5 4\n",
      "Decoded Labels: 0 1 4 s 5 9 am 0 0 5 x 4 2 2 0\n",
      "Decoded Labels: m 1 2 6 5 zrzbm 8 8 4\n",
      "Decoded Labels: kcb 5 3 5 pcos 1 7 x\n",
      "Decoded Labels: 0 1 2 j 0 3 a 3 2 3 6 x 0 8 3 5\n",
      "Decoded Labels: 0 2 jhb 0 7 0 i 6 1 2 c 0 8 4\n",
      "Decoded Labels: m 0 5 6 5 stbhm 0 4 2\n",
      "Decoded Labels: s 7 4 0 8 csbwm 9 1 1\n",
      "Decoded Labels: m 0 5 3 1 opbim 3 1 e\n",
      "Decoded Labels: kdq 7 8 8 sqps 9 0 0\n",
      "Decoded Labels: 1 5 6 m 8 8 af 0 1 0 x 1 4 6 0\n",
      "Decoded Labels: s 9 2 1 9 pmetm 1 9 6\n",
      "Decoded Labels: m 5 3 1 soaawxm 7 9 b\n",
      "Decoded Labels: kcq 2 4 4 lucs 7 0 k\n",
      "Decoded Labels: m 9 2 2 0 uwbvm 9 1 8\n",
      "Decoded Labels: e 1 5 8 8 trilqd 3 0 7\n",
      "Decoded Labels: 2 4 1 l 0 6 a 6 1 0 2 x 9 0 0 0\n",
      "Decoded Labels: cd 1 3 3 5 6 za 0 0 4 0 0 0 0\n",
      "Decoded Labels: m 0 4 4 7 cangm 2 2 l\n",
      "Decoded Labels: s 0 8 5 6 owcbm 9 0 0\n",
      "Decoded Labels: b 0 9 6 1 adrcod 3 5 6\n",
      "Decoded Labels: b 1 6 7 7 wommtd 3 0 0\n",
      "Decoded Labels: m 9 3 2 7 umolm 8 1 p\n",
      "Decoded Labels: m 0 6 8 6 wjiyxm 8 8 5\n",
      "Decoded Labels: cdbc 0 4 1 1 4 0 0 0 0\n",
      "Decoded Labels: m 0 5 6 5 pazexm 5 4 h\n",
      "Decoded Labels: m 0 5 3 8 ocalm 2 2 y\n",
      "Decoded Labels: m 0 5 3 1 nwddm 9 0 0\n",
      "Decoded Labels: e 0 8 2 2 trilqd 9 1 0\n",
      "Decoded Labels: kcq 5 4 9 fwls 9 0 0\n",
      "Decoded Labels: 2 4 1 v 2 9 a 6 7 4 6 x 9 6 3 7\n",
      "Decoded Labels: s 0 8 4 1 ovrbm 7 4 p\n",
      "Decoded Labels: kci 6 7 5 scrs 4 6 x\n",
      "Decoded Labels: cd 1 1 2 1 1 0 m 0 0 2 0 0 0 0\n",
      "Decoded Labels: s 0 2 0 4 oahkxm 1 6 y\n",
      "Decoded Labels: m 9 2 4 2 bwdem 0 9 0\n",
      "Decoded Labels: m 5 3 1 spcymxm 4 2 8\n",
      "Decoded Labels: kcq 3 6 9 veas 9 0 0\n",
      "Decoded Labels: s 2 0 4 sjahkxm 5 5 h\n",
      "Decoded Labels: m 5 6 5 soazexm 2 5 3\n",
      "Decoded Labels: e 0 9 7 0 rndcyd 3 0 1\n",
      "Decoded Labels: 2 4 1 r 3 6 a 7 6 0 8 x 5 8 3 0\n",
      "Decoded Labels: 8 5 gin 1 0 6 i 6 0 1 c 5 4 0\n",
      "Decoded Labels: kcq 2 0 9 dlps 2 7 g\n",
      "Decoded Labels: m 5 3 1 sochdxm 0 3 0\n",
      "Decoded Labels: kdb 7 5 6 pgps 9 0 0\n",
      "Decoded Labels: kcq 2 6 6 veas 3 3 h\n",
      "Decoded Labels: m 0 5 0 5 ocalm 2 0 y\n",
      "Decoded Labels: kci 5 4 4 twws 1 1 x\n",
      "Decoded Labels: cd 1 5 3 b 6 x 1 0 1 6 0 0 0 0\n",
      "Decoded Labels: m 9 2 3 0 umosm 5 3 e\n",
      "Decoded Labels: m 0 4 4 6 ifvkm 5 1 r\n",
      "Decoded Labels: m 0 4 4 7 caatxm 6 6 f\n",
      "Decoded Labels: 9 4 4 e 5 0 am 5 1 0 x 0 8 5 4\n",
      "Decoded Labels: kcl 0 2 4 cfms 9 0 0\n",
      "Decoded Labels: kcp 9 2 8 srys 3 4 r\n",
      "Decoded Labels: 1 2 dst 1 4 0 i 9 2 7 c 5 2 6\n",
      "Decoded Labels: s 0 8 5 6 owcbm 0 3 0\n",
      "Decoded Labels: m 0 5 3 8 iloim 5 0 p\n",
      "Decoded Labels: kcq 7 0 7 lcss 6 2 k\n",
      "Decoded Labels: kdp 9 1 6 lnys 2 7 g\n",
      "Decoded Labels: b 0 0 4 6 unpym 9 0 0\n",
      "Decoded Labels: 1 5 7 c 0 9 a 2 9 8 2 x 0 8 7 9\n",
      "Decoded Labels: m 4 4 7 svahlxm 9 0 0\n",
      "Decoded Labels: s 5 4 8 0 vtdtm 8 0 8\n",
      "Decoded Labels: 0 2 1 b 3 9 a 7 0 1 8 x 5 8 4 0\n",
      "Decoded Labels: m 1 2 8 6 zrhmm 9 1 8\n",
      "Decoded Labels: 0 5 7 c 1 9 a 1 1 1 6 x 5 6 4 0\n",
      "Decoded Labels: m 0 4 4 6 vahlxm 7 9 b\n",
      "Decoded Labels: m 0 5 0 0 oaawxm 7 9 b\n",
      "Decoded Labels: v 0 5 4 5 abccyd 3 0 1\n",
      "Decoded Labels: m 0 5 6 6 ongem 4 3 m\n",
      "Decoded Labels: s 5 6 5 2 ccehm 9 0 0\n",
      "Decoded Labels: 2 4 1 r 3 0 a 1 2 1 1 x 9 0 0 0\n",
      "Decoded Labels: s 5 4 4 0 srmom 9 2 8\n",
      "Decoded Labels: m 5 3 1 spazexm 9 0 0\n",
      "Decoded Labels: m 0 4 5 5 btixm 9 1 1\n",
      "Decoded Labels: 9 5 dam 0 5 5 i 9 0 4 c 9 0 6\n",
      "Decoded Labels: s 8 5 1 8 onmjm 0 3 0\n",
      "Decoded Labels: s 2 0 8 9 uskcm 0 7 j\n",
      "Decoded Labels: kcc 2 0 0 vnrs 4 1 p\n",
      "Decoded Labels: cd 1 2 4 5 e 0 c 0 0 1 0 0 0 0\n",
      "Decoded Labels: m 0 5 0 5 oceam 0 1 h\n",
      "Decoded Labels: m 0 5 6 5 jaawxm 6 1 p\n",
      "Decoded Labels: m 5 0 5 spchfxm 0 3 0\n",
      "Decoded Labels: kcb 7 1 0 ccus 2 4 r\n",
      "Decoded Labels: 0 1 7 m 4 4 a 2 9 6 0 x 9 0 0 0\n",
      "Accuracy: 96.46%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        images, texts = batch\n",
    "        \n",
    "        # Move tensors to the correct device\n",
    "        images = images.to(device)\n",
    "        texts = {key: value.squeeze(1).to(device) for key, value in texts.items()}\n",
    "        \n",
    "        # Decode the labels\n",
    "        #decoded_labels = [processor.tokenizer.decode(value[0], skip_special_tokens=True) for value in texts.values()]\n",
    "        #print(f\"Decoded Labels: {decoded_labels[0]}\")\n",
    "        \n",
    "        # Model inference\n",
    "        outputs = model(**texts, pixel_values=images)\n",
    "        logits_per_image = outputs.logits_per_image  # Image-text similarity\n",
    "        \n",
    "        # Predictions\n",
    "        preds = logits_per_image.argmax(dim=-1)\n",
    "        labels = torch.arange(len(images)).to(device)\n",
    "        \n",
    "        # Print predicted and actual labels\n",
    "        # for idx, pred_idx in enumerate(preds):\n",
    "        #     print(f\"Image {idx + 1}:\")\n",
    "        #     print(f\"  Actual: {decoded_labels[idx]}\")\n",
    "        #     print(f\"  Predicted: {decoded_labels[pred_idx]}\")\n",
    "        \n",
    "        # Accuracy calculation\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += len(images)\n",
    "\n",
    "print(f\"Accuracy: {correct / total:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
